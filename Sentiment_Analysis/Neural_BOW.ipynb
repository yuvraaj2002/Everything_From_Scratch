{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is sentiment analysis?\n",
    "\n",
    "Sentiment analysis, also known as opinion mining, is a natural language processing (NLP) task that involves determining the emotional tone expressed in a piece of text. The primary goal of sentiment analysis is to classify the text into different categories based on the sentiment it conveys, such as positive, negative, or neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything imported succesfullyüëçüèª\n"
     ]
    }
   ],
   "source": [
    "# Importing collections module for handling collections of data\n",
    "import collections\n",
    "\n",
    "# Importing datasets module for loading and processing datasets\n",
    "import datasets\n",
    "\n",
    "# Importing matplotlib.pyplot for plotting graphs and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing numpy for numerical operations and array manipulations\n",
    "import numpy as np\n",
    "\n",
    "# Importing torch for building and training neural networks\n",
    "import torch\n",
    "\n",
    "# Importing torch.nn for defining neural network layers and functions\n",
    "import torch.nn as nn\n",
    "\n",
    "# Importing torch.optim for optimization algorithms\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# For tokenization\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# Importing stopwords from nltk.corpus for removing common words that do not contribute to the sentiment\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Storing the stopwords in a set for faster lookup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Importing tqdm for progress bar visualization\n",
    "import tqdm\n",
    "\n",
    "import string\n",
    "\n",
    "print(\"Everything imported succesfullyüëçüèª\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.25\n",
    "max_length = 256\n",
    "min_freq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.81k/7.81k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.0M/21.0M [00:03<00:00, 6.41MB/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:03<00:00, 6.60MB/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42.0M/42.0M [00:05<00:00, 8.06MB/s]\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 89288.92 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 110008.76 examples/s]\n",
      "Generating unsupervised split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:00<00:00, 105918.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Let's check the features in the data\n",
    "print(train_data.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\'t go on to star in more and better films. Sadly, I didn\\'t think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat\\'s Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\\'s ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain\\'t no \"Paper Moon\" and only a very pale version of \"What\\'s Up, Doc\".',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "tokenizer: the name of tokenizer function. If None, it returns split()\n",
    "            function, which splits the string sentence by space.\n",
    "            If basic_english, it returns _basic_english_normalize() function,\n",
    "            which normalize the string first and split by space. If a callable\n",
    "            function, it will return the function. If a tokenizer library\n",
    "            (e.g. spacy, moses, toktok, revtok, subword), it returns the\n",
    "            corresponding library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'yuvraj', 'singh']\n"
     ]
    }
   ],
   "source": [
    "# For tokenization\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "print(tokenizer(\"My name is Yuvraj Singh\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(raw_text, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text using the specified tokenizer, removes stop words and punctuation, and truncates the tokens to the maximum length.\n",
    "\n",
    "    Args:\n",
    "        raw_text (dict): A dictionary containing the text to be tokenized with the key \"text\".\n",
    "        tokenizer (callable): A tokenizer function that takes a string and returns a list of tokens.\n",
    "        max_length (int): The maximum number of tokens to return.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the truncated list of tokens with the key \"tokens\".\n",
    "    \"\"\"\n",
    "    tokens = [token for token in tokenizer(raw_text[\"text\"]) if token not in stop_words and token not in string.punctuation][:max_length]\n",
    "    return {\"tokens\": tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset provided by the datasets library is an instance of a Dataset class. We can see all the methods in a Dataset here, but the main one we are interested in is map. By using map we can apply a function to every example in the dataset and either update the example or create a new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['hello',\n",
       "  'test',\n",
       "  'sentence',\n",
       "  'lot',\n",
       "  'punctuation',\n",
       "  'stop',\n",
       "  'words',\n",
       "  'like',\n",
       "  'let',\n",
       "  'see',\n",
       "  'works']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_text = {\n",
    "    \"text\": \"Hello!!! This is a test sentence, with a lot of punctuation... and some stop words like 'the', 'is', 'at', 'which', and 'on'. Let's see how it works!\"\n",
    "}\n",
    "\n",
    "tokenize_sentence(dummy_text, tokenizer,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:10<00:00, 2448.57 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:09<00:00, 2538.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.map(\n",
    "    tokenize_sentence, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")\n",
    "test_data = test_data.map(\n",
    "    tokenize_sentence, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'tokens'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of validation set\n",
    "\n",
    "Talk about the differnece between train,test and validation split\n",
    "\n",
    "We can split a Dataset using the train_test_split method which splits a dataset into two, creating a DatasetDict for each split, one called train and another called test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_data = train_data.train_test_split(test_size=test_size)\n",
    "train_data = train_valid_data[\"train\"]\n",
    "valid_data = train_valid_data[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25,000 training examples have now been split into 18,750 training examples and 6,250 validation examples, with the original 25,000 test examples remaining untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14062, 4688, 25000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Vocabulary\n",
    "\n",
    "Next, we have to build a vocabulary. This is look-up table where every unique token in your dataset has a corresponding index (an integer).\n",
    "\n",
    "We do this as machine learning models cannot operate on strings, only numerical vaslues. Each index is used to construct a one-hot vector for each token. A one-hot vector is a vector where all the elements are 0, except one, which is 1, and the dimensionality is the total number of unique tokens in your vocabulary, commonly denoted by \n",
    ".\n",
    "\n",
    "For example:\n",
    "\n",
    "\n",
    "\n",
    "One issue with creating a vocabulary using every single word in the dataset is that there are usually a considerable amount of unique tokens. One way to combat this is to either only construct the vocabulary only using the most commonly appearing tokens, or to only use tokens which appear a minimum amount of times in the dataset. In this notebook, we do the latter, keeping on the tokens which appear 5 times.\n",
    "\n",
    "What happens to tokens which appear less than 5 times? We replace them with a special unknown token, denoted by <unk>. For example, if the sentence \"This film is great and I love it\", but the word \"love\" was not in the vocabulary, it would become: \"This film is great and I <unk> it\".\n",
    "\n",
    "We use the build_vocab_from_iterator function from torchtext.vocab to create our vocabulary, specifying the min_freq (the minimum amount of times a token should appear to be added to the vocabulary) and special_tokens (tokens which should be appended to the start of the vocabulary, even if they don't appear min_freq times in the dataset).\n",
    "\n",
    "The first special token is our unknown token, the other, <pad> is a special token we'll use for padding sentences.\n",
    "\n",
    "When we feed sentences into our model, we pass a batch of sentences, i.e. more than one, at the same time. Passing a batch of sentences is preferred to passing sentences one at a time as it allows our model to perform computation on all sentences within a batch in paralle, thus speeding up the time taken to train and evaluate our model. All sentences within a batch need to be the same length (in terms of the number of tokens). Thus, to ensure each sentence is the same length, any shorter than the longest sentence need to have padding tokens appended to the end of them.\n",
    "\n",
    "For an example batch of two sentences of length four and three tokens:\n",
    "\n",
    "As we can see, the second sentence has been padded with a single <pad> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    train_data[\"tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary :  21734\n",
      "First 10 tokens in vocabulary :  ['<unk>', '<pad>', 'movie', 'film', 'one', 'like', 'good', 'even', 'time', 'would']\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of vocabulary : \",len(vocab))\n",
    "print(\"First 10 tokens in vocabulary : \",vocab.get_itos()[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
