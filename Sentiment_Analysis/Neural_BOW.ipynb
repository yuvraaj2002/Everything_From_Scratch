{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is sentiment analysis?\n",
    "\n",
    "Sentiment analysis, also known as opinion mining, is a natural language processing (NLP) task that involves determining the emotional tone expressed in a piece of text. The primary goal of sentiment analysis is to classify the text into different categories based on the sentiment it conveys, such as positive, negative, or neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything imported succesfullyüëçüèª\n"
     ]
    }
   ],
   "source": [
    "# Importing collections module for handling collections of data\n",
    "import collections\n",
    "\n",
    "# Importing datasets module for loading and processing datasets\n",
    "import datasets\n",
    "\n",
    "# Importing matplotlib.pyplot for plotting graphs and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing numpy for numerical operations and array manipulations\n",
    "import numpy as np\n",
    "\n",
    "# Importing torch for building and training neural networks\n",
    "import torch\n",
    "\n",
    "# Importing torch.nn for defining neural network layers and functions\n",
    "import torch.nn as nn\n",
    "\n",
    "# Importing torch.optim for optimization algorithms\n",
    "import torch.optim as optim\n",
    "\n",
    "# For tokenization\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# Importing stopwords from nltk.corpus for removing common words that do not contribute to the sentiment\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Storing the stopwords in a set for faster lookup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Importing tqdm for progress bar visualization\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "print(\"Everything imported succesfullyüëçüèª\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.81k/7.81k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.0M/21.0M [00:03<00:00, 6.41MB/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:03<00:00, 6.60MB/s]\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42.0M/42.0M [00:05<00:00, 8.06MB/s]\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 89288.92 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25000/25000 [00:00<00:00, 110008.76 examples/s]\n",
      "Generating unsupervised split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:00<00:00, 105918.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Let's check the features in the data\n",
    "print(train_data.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\'t go on to star in more and better films. Sadly, I didn\\'t think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat\\'s Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\\'s ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain\\'t no \"Paper Moon\" and only a very pale version of \"What\\'s Up, Doc\".',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "tokenizer: the name of tokenizer function. If None, it returns split()\n",
    "            function, which splits the string sentence by space.\n",
    "            If basic_english, it returns _basic_english_normalize() function,\n",
    "            which normalize the string first and split by space. If a callable\n",
    "            function, it will return the function. If a tokenizer library\n",
    "            (e.g. spacy, moses, toktok, revtok, subword), it returns the\n",
    "            corresponding library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'yuvraj', 'singh']\n"
     ]
    }
   ],
   "source": [
    "# For tokenization\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "print(tokenizer(\"My name is Yuvraj Singh\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(raw_text, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text using the specified tokenizer, removes stop words and punctuation, and truncates the tokens to the maximum length.\n",
    "\n",
    "    Args:\n",
    "        raw_text (dict): A dictionary containing the text to be tokenized with the key \"text\".\n",
    "        tokenizer (callable): A tokenizer function that takes a string and returns a list of tokens.\n",
    "        max_length (int): The maximum number of tokens to return.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the truncated list of tokens with the key \"tokens\".\n",
    "    \"\"\"\n",
    "    tokens = [token for token in tokenizer(raw_text[\"text\"]) if token not in stop_words and token not in string.punctuation][:max_length]\n",
    "    return {\"tokens\": tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset provided by the datasets library is an instance of a Dataset class. We can see all the methods in a Dataset here, but the main one we are interested in is map. By using map we can apply a function to every example in the dataset and either update the example or create a new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['hello',\n",
       "  'test',\n",
       "  'sentence',\n",
       "  'lot',\n",
       "  'punctuation',\n",
       "  'stop',\n",
       "  'words',\n",
       "  'like',\n",
       "  'let',\n",
       "  'see',\n",
       "  'works']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_text = {\n",
    "    \"text\": \"Hello!!! This is a test sentence, with a lot of punctuation... and some stop words like 'the', 'is', 'at', 'which', and 'on'. Let's see how it works!\"\n",
    "}\n",
    "\n",
    "tokenize_sentence(dummy_text, tokenizer,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "train_data = train_data.map(\n",
    "    tokenize_sentence, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")\n",
    "test_data = test_data.map(\n",
    "    tokenize_sentence, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
